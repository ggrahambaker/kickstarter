{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does Your Kickstarter Suck?\n",
    "Analysis of kickstarter data to predict success of campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "Crowdsourcing is an increasingly popular way for entreuprenuers to raise money for a product, project or somethig. Kickstarter has emereged as the top funding platform for this purpose. I was interested in what makes a kickstarter campaign successful, specifically, what attributes are common in the most successful campaigns. The attibutes I wanted to look at in the campaigns were the main category, sub category, the funding goal, whether or not it was selected as a \"staff pick,\" and the contents of the description of the product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling and Cleaning\n",
    "I obtained my data from webrobots.io, where they posted archived kickstarter data they have been scraping from the kickstarter website since 2015. There was over 40 gb of csv files archived on the site. The csv files contained json in serveral columns, as well as regularly typed data. I had to pull out relvant information from the json, as well as delete columns I determined didn't provide insight into the analysis. I also decided to work only on projects from the US, which would make the analysis easier for comparison against one another. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "### Logistic Regression (Logit)\n",
    "Logistic Regression is useful because it creates coefficients for each feature, meaning I can see which feature effects the model on the whole more clearly. \n",
    "### Random Forest\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0 = pd.read_csv('0.1.csv')\n",
    "#df1 = pd.read_csv('1.1.csv')\n",
    "#df2 = pd.read_csv('2.1.csv')\n",
    "#df3 = pd.read_csv('3.1.csv')\n",
    "df4 = pd.read_csv('4.1.csv')\n",
    "#arr = [df0, df1, df2, df3, df4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_df = df4 #pd.concat(arr)\n",
    "super_df = super_df.set_index('id')\n",
    "super_df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a few of these fields are relevant to our analysis. We will create a new dataframe with only the important fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = super_df[['country', 'name', 'blurb', 'goal', 'state', 'deadline', 'launched_at', 'main_category', 'sub_category', 'staff_pick']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill empty blurbs\n",
    "df['blurb'] = df['blurb'].fillna('')\n",
    "df = df[df['blurb'] != 'False']\n",
    "# take out all that are not live or suspended or canceled, \n",
    "# later 2 categories imply exceptional curcumstance\n",
    "# and kickstarter admistrative involvement\n",
    "df = df[df['state'] != 'live']\n",
    "df = df[df['state'] != 'canceled']\n",
    "df = df[df['state'] != 'suspended']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# only campaigns in the US\n",
    "df = df[df['country'] == 'US']\n",
    "# encode TRUE OR FALSE with 1 or 0\n",
    "df['staff_pick'] = (df.staff_pick == True).values.astype(np.int)\n",
    "# encode state as 1 for success, 0 for failure\n",
    "def encoder_(x):\n",
    "    if x['state'] == 'failed':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "df['state'] = df.apply(encoder_, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['launched_at_month'] = pd.DatetimeIndex(df['launched_at']).month\n",
    "df['deadline_month'] = pd.DatetimeIndex(df['deadline']).month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "# Import other preprocessing modules\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MaxAbsScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, roc_curve, auc\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "\n",
    "NUMERIC = ['main_category_encoded', 'sub_category_encoded', 'launched_at_month', 'deadline_month', 'goal', 'staff_pick']\n",
    "STATE = ['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "# apply \"le.fit_transform\"\n",
    "df_encoded = df[['main_category', 'sub_category']].apply(le.fit_transform)\n",
    "df['main_category_encoded'] = df_encoded['main_category']\n",
    "df['sub_category_encoded'] = df_encoded['sub_category']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "We first want to use a simple logistic regression to determine the most influencial fields in determining if a kickstarter campaign is a success or failure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[NUMERIC], df[STATE], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression()\n",
    "rfe = RFE(logit)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "#print(rfe.support_)\n",
    "#print(rfe.ranking_)\n",
    "support = np.array(rfe.support_)\n",
    "ranking = np.array(rfe.ranking_)\n",
    "# support\n",
    "var_importance = pd.DataFrame({'support': support, 'ranking': ranking})\n",
    "var_importance = var_importance.T\n",
    "var_importance.columns = NUMERIC\n",
    "var_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis, we find that main category, launched at month, and staff pick are the most important attributes for predicting success or failure. This make sense because some categories are typically more successful than others, with sub category coming closely behind in the importance ratings.  We also see that the month that the campaign was lauched in is a big factor in prediciting success. This is interesting because the deadline month in not nearly as influential as the launch month. We will take a closer look at this relationship later. Staff pick is also a good indicator because it creates more exposure for the campaign, and adds some level of endorsement of the project. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = rfe.predict(X_test)\n",
    "score = rfe.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model is 60 percent accurate, meaning the model performs slightly better than guessing if it will be a success or failure. A naive model would simply flip a coin to determine if it would be a success, giving it a 50 percent accuracy rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier for successful campaigns has high precision and low recall, meaning that it is very picky about which things it classifies as successful, but ultimately loses a lot of true successes because it is so strict. On the failed campaign side, its the exact opposite. Our classifier casts a wide net to identify failed campaigns, and identifies 96 percent of them, but is only right 56 percent of the time. Hopefully some parameter tuning can impove the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs = rfe.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our classifier is just above random chance. Hopefully we can improve this with some parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning parameters for logistic regression\n",
    "logit_params = {'C': [.01, .1, 1., 10., 100.], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "logit = LogisticRegression(class_weight='balanced')\n",
    "lgt = GridSearchCV(logit, logit_params, cv=10)\n",
    "lgt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model accuracy\n",
    "print('Acc: ', lgt.score(X_test, y_test))\n",
    "\n",
    "\n",
    "print(lgt.best_params_)\n",
    "C = lgt.best_params_['C']\n",
    "solver = lgt.best_params_['solver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=C, solver=solver)\n",
    "logit = logit.fit(X_train, y_train)\n",
    "#print(rfe.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=C, solver=solver)\n",
    "logit = logit.fit(X_train, y_train)\n",
    "\n",
    "pred = logit.predict(X_test)\n",
    "score = logit.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = rfe.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the paramaters only led to a 2 percent increase in AUC. Maybe its time to try a different classifier to see if we can improve it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[NUMERIC], df[STATE], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfe = RFE(rfc)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "#print(rfe.support_)\n",
    "#print(rfe.ranking_)\n",
    "support = np.array(rfe.support_)\n",
    "ranking = np.array(rfe.ranking_)\n",
    "# support\n",
    "var_importance = pd.DataFrame({'support': support, 'ranking': ranking})\n",
    "var_importance = var_importance.T\n",
    "var_importance.columns = NUMERIC\n",
    "var_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are suprising because it almost exactly the inverse of the importance fields for Logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "# Fit the pipeline to the training data\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "# Compute and print accuracy\n",
    "accuracy = rfc.score(X_test, y_test)\n",
    "print(\"\\RandomForestClassifier accuracy: \", accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the box, our random forest classifier is performing better than our logistic classifier. The f1 scores for sucessful and failed campaigns both are around 70, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "probs = rfc.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try tuning paramters!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [1, 2, 4, 8, 16]\n",
    "max_depths = np.linspace(1, 12, 6, endpoint=True)\n",
    "min_samples_splits = np.linspace(0.5, 1.0, 5, endpoint=True)\n",
    "\n",
    "\n",
    "\n",
    "# Tuning parameters for logistic regression\n",
    "rfc_params = {'max_depth': max_depths, \n",
    "                'min_samples_split': min_samples_splits}\n",
    "rfc = RandomForestClassifier()\n",
    "rfc = GridSearchCV(rfc, rfc_params, cv=5)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model accuracy\n",
    "print('Acc: ', rfc.score(X_test, y_test))\n",
    "\n",
    "\n",
    "print(rfc.best_params_)\n",
    "max_depth = rfc.best_params_['max_depth']\n",
    "min_samples_split = rfc.best_params_['min_samples_split']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=max_depth, min_samples_split=min_samples_split, n_estimators=n_estimator)\n",
    "rfc = rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "# Compute and print accuracy\n",
    "accuracy = rfc.score(X_test, y_test)\n",
    "print(\"\\RandomForestClassifier accuracy: \", accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = rfc.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[NUMERIC], df[STATE], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "rfe = RFE(mnb)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "#print(rfe.support_)\n",
    "#print(rfe.ranking_)\n",
    "support = np.array(rfe.support_)\n",
    "ranking = np.array(rfe.ranking_)\n",
    "# support\n",
    "var_importance = pd.DataFrame({'support': support, 'ranking': ranking})\n",
    "var_importance = var_importance.T\n",
    "var_importance.columns = NUMERIC\n",
    "var_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "# Compute and print accuracy\n",
    "accuracy = mnb.score(X_test, y_test)\n",
    "print(\"\\nMultinomialNB: \", accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = mnb.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning parameters for bayes regression\n",
    "bayes_params = {'alpha': [.01, .1, 1., 10., 100.]}\n",
    "mnb = MultinomialNB()\n",
    "mnb = GridSearchCV(mnb, bayes_params, cv=10)\n",
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model accuracy\n",
    "print('Acc: ', mnb.score(X_test, y_test))\n",
    "\n",
    "\n",
    "print(mnb.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnb = MultinomialNB(alpha=100)\n",
    "\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "# Compute and print accuracy\n",
    "accuracy = mnb.score(X_test, y_test)\n",
    "print(\"\\nMultinomialNB: \", accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = mnb.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[NUMERIC], df[STATE], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "rfe = RFE(svc)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "#print(rfe.support_)\n",
    "#print(rfe.ranking_)\n",
    "support = np.array(rfe.support_)\n",
    "ranking = np.array(rfe.ranking_)\n",
    "# support\n",
    "var_importance = pd.DataFrame({'support': support, 'ranking': ranking})\n",
    "var_importance = var_importance.T\n",
    "var_importance.columns = NUMERIC\n",
    "var_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "# Compute and print accuracy\n",
    "accuracy = svc.score(X_test, y_test)\n",
    "print(\"\\nSVM: \", accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = mnb.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning parameters for logistic regression\n",
    "parameters = {'C':[1, 10, 100],\n",
    "              'gamma':[0.1, 0.01]}\n",
    "svc = SVC()\n",
    "svc = GridSearchCV(svc, parameters, cv=10)\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model accuracy\n",
    "print('Acc: ', svc.score(X_test, y_test))\n",
    "\n",
    "\n",
    "print(svc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "# Compute and print accuracy\n",
    "accuracy = mnb.score(X_test, y_test)\n",
    "print(\"\\nMultinomialNB: \", accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = mnb.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
